import logging
import asyncio
from typing import Union
from pydantic import BaseModel, Field

from utils.config import *
from utils.utils import *
from utils.utils import get_local_data

logger = logging.getLogger(__name__)

date, day = get_local_data()


async def query_agent(query, llm, date, day):
    """
    Generates a list of subqueries based on a given query using an LLM agent.

    Args:
        query (str): The initial query.
        llm: The language model instance used for generating content.
        date (str): The current date.
        day (str): The current day of the week.

    Returns:
        list: A list of subqueries generated by the LLM, cleaned and formatted.
    """

    class SearchSubqueries(BaseModel):
        """Generates a list of subqueries based on a given query using an LLM agent.
        - Use date/location only if the query requires time-sensitive or location-specific information.
        Replace time-specific words with context details.
        Rephrase the query for search engine optimization and clarity.
        Avoid including the date if not necessary.
        Emphasize date or location when needed.
        When breaking down into multiple tasks, always consider the overall task to ensure every aspect is included. By the end of searching for subqueries, there should be enough information to process the answer.
        If necessary, split the query into up to 2 diverse subqueries to cover all aspects.
        Subqueries should not be in a numbered list.
        Plan searches to answer every part of the query without overemphasizing any term.
        Ensure that subqueries include essential keywords and context from the original query to maintain relevance. Every query will be searched independently, so ensure that each subquery is self-contained and can be understood without additional context.
        Avoid generating subqueries that are too generic or lack specific terms from the original query.
        When rephrasing, retain the main subject and important details from the original query in each subquery.
        CHECK FOR NEXT STEPS OR INCOMPLETE INFORMATION ELEMENTS IN EARLIER RESPONSE OR QUERY, IF THERE ARE ANY PLAN ACCORDINGLY.
        """ + "Todays date is: {date} and today is {day}, {place}, Use date/location only if the query requires time-sensitive or location-specific information.",

        planning_to_answer_query_to_help_finding_subqueries: list[str] = Field(
            description="Up to 6 sub-tasks, breaking down the task without overemphasizing any keyword, Context: Today's date is {date} {day}, {place}. These tasks are being given to different agents, so each task description should be self sufficient with all details. Avoid sequntial tasks, break it down into parallel tasks."
        )
        subqueries: list[str] = Field(
            description="Up to 6 rephrased phrases covering all planning steps in independent search phrases , using a maximum of 6 words per subquery, include all details for the subtasks based on query, Context: Today's date is {date} {day}, {place}"
        )
        is_summary: bool = Field(
            description="Whether the subqueries are intended to summarize. Generally, for the subqueries focused on structured data, tables, list, codes. You SHOULD use summarise!"
        )
        is_structured_data: bool = Field(
            description= "Whether the query will hit a page with structured data like tables, codes, list, indexes, where full page full page would be needed to access to answer the query OR is it the case chunking page content will leave give only half of required information for a given query"
        )
        is_covered_urls: bool = Field(  
            description="If user is asking to answer explicitly using youtube or reddit"
        )

    logger = logging.getLogger(__name__)
    try:
        response = await llm.with_structured_output(SearchSubqueries).ainvoke(
            f"Todays date is: {date} and today is {day}, {place}, Use date/location only if the query requires time-sensitive or location-specific information. "
            f"\nquery: {query}"
        )
        is_summary = response.is_summary
        if response.is_structured_data:
            is_summary = True
        is_covered_urls = response.is_covered_urls
        response = response.subqueries
        logger.info(f"Summary:{is_summary},subquery:{response}, iscoverd:{is_covered_urls}")
    except Exception as e:
        logger.warning(f"Structured output failed: {e}. Falling back to prompt-based extraction.")
        try:
            prompt = prompts['query_agent_basic'].format(date=date, day=day, location=location, query=query)
            response = await llm.ainvoke(prompt)
            response = response.content
            response = extract_subqueries(response.text)
        except Exception as ex:
            logger.error(f"Both structured and prompt-based extraction failed: {ex}")
            return []
    try:
        response = [
            r.lower().replace('*', '').replace("subquery", "").replace("'", "")
            for r in response if r != ''
        ]
    except Exception as e:
        logger.error(f"Error cleaning subqueries: {e}")
        return []
    return response, is_summary,is_covered_urls


async def response_gen(model, query, context):
    """
    Generates a comprehensive response for a given query using an LLM, incorporating context.

    Args:
        model: The language model instance used for generating content.
        query (str): The query for which a response is needed.
        context (str): The context to be included in the response generation.

    Returns:
        tuple: The synthesized answer and sources formatted in markdown.
    """

    class ResponseGen(BaseModel):
        f"""Generates a comprehensive response for a given query using an LLM, incorporating context.
        Instructions:
        You are an AI assistant that generates a comprehensive response to a user query using the provided context.

        Understand the query thoroughly, even if it contains misspellings or errors. Use the provided context and related information to interpret the intended meaning.

        Answer the query using the context and available information; if the context is insufficient, utilise whatever information is relevant, its fine even if answer is incomplete. utilize your own knowledge to provide a complete answer, including code if requested.

        Dont just provide what is their in the web search results, but synthesize the information from the context and your own knowledge to create a comprehensive answer. If the query is about code, provide the code directly rather than a plan to implement it.

        Provide direct answers: If the query asks for code, supply the code rather than a plan to implement it. Learn facts, syntax, and structures from the context to create accurate responses.

        Enhance your response with engaging details, clear reasoning, and simple explanations. Use analogies to illustrate difficult concepts when appropriate.

        Cite sources for every detail using hyperlinks to the search result URLs; if using your own knowledge, cite as "llm_generated". Structure your answer in markdown format with bullet points. Utilize the context fully, even if it doesn't directly answer the question. Mention any next steps in next_steps; otherwise, write None.

        Query:
        {query}

        Context:
        {context}

        KEEP ANSWER CONCISE UNLESS ASKED FOR DETAILED.
        Summarize the context to answer user query, capture granularity to help customer and get value from the context.
        """
        intent_understanding: str = Field(
            description="Understanding of the query, including any misspellings or errors."
        )
        synthesized_answer_based_on_various_sources: str = Field(
            description="Detailed answer to the query based on all available information in the context, even if limited."
        )
        sources: list[str] = Field(
            description="List of sources used to answer the query, including hyperlinks to search results."
        )
        next_steps: Union[str, None] = Field(
            description="Next steps/searches needed to accomplish complete response to users query, if applicable (incomplete answers); and otherwise, None.( Tip:Many of the cases, summarising one link from sources would suffice)"
        )

    logger.info("Generating Answer for query: %s", query)

    try:
        response = await model.with_structured_output(ResponseGen).ainvoke(
            f"Query: {query}\nContext: {context}"
        )
        response = response.dict()
        sources = '\n'.join([f'[{i}]. ' + s + '\n' for i, s in enumerate(response['sources'])])
        answer = (
            "#### Answer: \n"
            + str(response['synthesized_answer_based_on_various_sources'])
            + "\n\n#### Next steps:\n"
            + str(response['next_steps'])
        )
        logger.info("Structured response generated successfully.")
    except Exception as e:
        logger.warning(f"Structured output failed: {e}. Falling back to prompt-based generation.")
        try:
            prompt = prompts['qa_response_generation'].format(context=context, query=query)
            prompt_template = f"{prompt}"
            response = await model.ainvoke(prompt_template)
            response = response.content
            answer = "#### Answer: \n" + response
            sources = ''
            logger.info("Fallback prompt-based response generated successfully.")
        except Exception as ex:
            logger.error(f"Both structured and prompt-based response generation failed: {ex}")
            return "Error: Unable to generate answer.", ""
    return answer, sources


async def summarizer(query, docs, llm, batch,max_docs=30,max_words_per_doc=6000):
    """
    Summarizes a list of documents iteratively in batches using an LLM.

    Args:
        query (str): The initial query guiding the summarization.
        docs (list): A list of documents to be summarized.
        llm: The language model instance used for generating content.
        batch (int): The number of documents to process in each batch.

    Returns:
        list: A summarized version of the input documents.
    """

    if not docs or not isinstance(docs, list):
        logger.warning("No documents provided or docs is not a list.")
        return []
    logger.info(f"Deduping docs: {len(docs)}")
    logger.warning(f"Capping max words to {max_words_per_doc}")
    docs = list(set([f'source:{k.metadata["source"]}\ncontent:{" ".join(k.page_content.split(" ")[:max_words_per_doc])}' for k in docs]))
    logger.info(f"Arrived Len of Docs: {len(docs)}")
    len_docs = len(docs)
    if len_docs == 1:
        logger.info("Only one document provided, summarizing it.")
        try:
            comb_docs = f'Document 0: {str(docs[0])}'
            prompt = prompts['summary_generation'].format(comb_docs=comb_docs, query=query)
            response = await llm.ainvoke(prompt)
            logger.info(f"Response type: {type(response)}")
            logger.info(f"Response attributes: {dir(response)}")
            logger.info(f"Response: {response}")
            
            # Handle different response types from async LLM calls
            if hasattr(response, 'content') and not callable(response.content):
                summary_text = response.content
            elif hasattr(response, 'text') and not callable(response.text):
                summary_text = response.text
            elif hasattr(response, 'content') and callable(response.content):
                # If content is a method, call it
                summary_text = response.content()
            elif hasattr(response, 'text') and callable(response.text):
                # If text is a method, call it
                summary_text = response.text()
            else:
                summary_text = str(response)
            
            if summary_text and isinstance(summary_text, str):
                return summary_text.strip()
            else:
                logger.warning("Empty or invalid summary for single document.")
                return 'SUMMARIZATION FAILED'
        except Exception as e:
            logger.error(f"Summarization failed for single document: {e}")
            return 'SUMMARIZATION FAILED'

    print(f"Summarising using {len_docs} documents")
    if len_docs > max_docs:
        logger.warning(f"Too many documents ({len_docs}) provided, limiting to {max_docs}. This may lead to loss of information.")
        docs = docs[:max_docs]
        len_docs = max_docs
    while len_docs > 1:
        summaries = []
        
        # Create async tasks for parallel batch processing
        async def process_batch(batch_docs, batch_index):
            if not batch_docs:
                return None
            
            comb_docs = '\n'.join(
                [f'Document {j+batch_index}:' + str(d) for j, d in enumerate(batch_docs)]
            )
            
            try:
                prompt = prompts['summary_generation'].format(comb_docs=comb_docs, query=query)
                response = await llm.ainvoke(prompt)
                # Handle different response types from async LLM calls
                if hasattr(response, 'content') and not callable(response.content):
                    summary_text = response.content
                elif hasattr(response, 'text') and not callable(response.text):
                    summary_text = response.text
                elif hasattr(response, 'content') and callable(response.content):
                    # If content is a method, call it
                    summary_text = response.content()
                elif hasattr(response, 'text') and callable(response.text):
                    # If text is a method, call it
                    summary_text = response.text()
                else:
                    summary_text = str(response)
                
                if summary_text and isinstance(summary_text, str):
                    return summary_text.strip()
                else:
                    logger.warning(f"Empty or invalid summary for batch starting at {batch_index}.")
                    return None
            except Exception as e:
                logger.error(f"Async summarization failed for batch starting at {batch_index}: {e}")
                return None
        
        # Create tasks for all batches
        batch_tasks = []
        for i in range(0, len_docs, batch):
            batch_docs = docs[i:i + batch]
            batch_tasks.append(process_batch(batch_docs, i))
        
        # Execute all batch tasks in parallel
        if batch_tasks:
            logger.info(f"Processing {len(batch_tasks)} batches in parallel")
            batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
            
            # Collect successful results
            for result in batch_results:
                if isinstance(result, Exception):
                    logger.error(f"Batch processing failed: {result}")
                elif result is not None:
                    summaries.append(result)

        if not summaries:
            logger.error("No summaries generated in this async iteration, aborting.")
            return 'SUMMARIZATION FAILED'

        len_docs = len(summaries)
        docs = summaries

    return '\n'.join(docs)
